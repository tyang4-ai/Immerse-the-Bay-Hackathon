# HoloHuman XR: Down the Rabbit Hole

**Immerse the Bay 2025 Hackathon Project**

Explore the human body layer-by-layer with AI-powered medical insights in virtual reality.

---

## ğŸ¯ Project Overview

HoloHuman XR is an immersive medical visualization experience for Meta Quest 2 that allows users to:
- **Peel back anatomical layers** (skin â†’ muscle â†’ skeleton â†’ heart)
- **Analyze ECG signals** using AI/ML (detects 6 cardiac conditions)
- **View medical imaging** (X-rays, CT scans) in 3D VR space
- **Feel haptic feedback** simulating heartbeat and injury sensations

**Theme:** "Down the Rabbit Hole" - Dive deeper into the hidden world inside the human body

**Competition Tracks:**
- ğŸ† Primary: **Virtuous Reality** (Social Good + XR)
- ğŸ¤– Secondary: **AI Horizons** (AI + XR)

---

## âœ¨ Features

### Interactive 3D Anatomy
- Life-sized human body models in VR
- Peelable layers: toggle visibility of skin, muscles, skeleton, and organs
- Grab, rotate, and scale models with VR controllers

### AI-Powered ECG Analysis
- Pre-trained neural network (TensorFlow/Keras)
- Detects 6 cardiac conditions:
  - 1st degree AV block
  - Right/Left Bundle Branch Block (RBBB/LBBB)
  - Sinus bradycardia/tachycardia
  - Atrial fibrillation
- Real-time predictions with confidence scores

### Medical Imaging Viewer
- View X-rays and CT scans in VR
- Navigate through image slices
- Zoom and pan controls
- Portal effect for "diving into" scan space

### Haptic Feedback
- Heartbeat pulse synchronized with heart rate (60-120 BPM)
- Fracture sensation when touching broken bones
- Quest controller vibration

---

## ğŸ› ï¸ Tech Stack

**Frontend (VR)**
- Unity 2022.3 LTS
- Unity XR Interaction Toolkit
- OpenXR Plugin (Meta Quest 2)
- C# scripting
- TextMeshPro UI

**Backend (AI/ML)**
- Python 3.8+
- Flask REST API
- TensorFlow 2.2
- Keras
- Pre-trained ECG model ([antonior92/automatic-ecg-diagnosis](https://github.com/antonior92/automatic-ecg-diagnosis))

**Hardware**
- Meta Quest 2
- Quest Link (USB-C) for development

**Assets**
- 3D models from Sketchfab, TurboSquid (low-poly, VR-optimized)
- DICOM medical imaging samples (converted to PNG)

---

## ğŸ“ Project Structure

```
Immerse-the-Bay-Hackathon/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ RESOURCES.md                        # All resource links and tools
â”œâ”€â”€ Reference/                          # Hackathon rules and dev docs
â”œâ”€â”€ dev/active/holohuman-xr/           # Development documentation
â”‚   â”œâ”€â”€ holohuman-xr-plan.md           # Comprehensive dev plan
â”‚   â”œâ”€â”€ holohuman-xr-context.md        # Technical context and architecture
â”‚   â””â”€â”€ holohuman-xr-tasks.md          # Task checklist (165+ tasks)
â”œâ”€â”€ .claude/                            # Custom AI agents and commands
â”‚   â”œâ”€â”€ agents/                        # Documentation, code review, research agents
â”‚   â”œâ”€â”€ commands/                      # Dev workflow commands
â”‚   â””â”€â”€ hooks/                         # Automated skill activation
â”œâ”€â”€ UnityProject/                       # Unity VR application (TBD)
â””â”€â”€ Backend/                            # Flask API for ECG analysis (TBD)
```

---

## ğŸš€ Getting Started

### Prerequisites
- Unity 2022.3 LTS with Android Build Support
- Python 3.8+
- Meta Quest 2 with Developer Mode enabled
- USB-C cable for Quest Link

### Unity Setup

1. **Clone this repository:**
   ```bash
   git clone https://github.com/tyang4-ai/Immerse-the-Bay-Hackathon.git
   cd Immerse-the-Bay-Hackathon
   ```

2. **Clone Unity MR Example template:**
   ```bash
   cd UnityProject
   git clone https://github.com/Unity-Technologies/mr-example-meta-openxr .
   ```

3. **Open project in Unity Hub:**
   - Add project from `UnityProject` folder
   - Unity will import and configure packages

4. **Configure for Quest 2:**
   - File â†’ Build Settings â†’ Android
   - Edit â†’ Project Settings â†’ XR Plug-in Management
   - Enable OpenXR

5. **Connect Quest 2:**
   - Enable Developer Mode via Meta app
   - Connect via USB-C
   - Build and Run (or use Quest Link for faster iteration)

### Backend Setup

1. **Create Python virtual environment:**
   ```bash
   cd Backend
   python -m venv venv

   # Windows
   venv\Scripts\activate

   # Mac/Linux
   source venv/bin/activate
   ```

2. **Clone ECG model repository:**
   ```bash
   git clone https://github.com/antonior92/automatic-ecg-diagnosis
   cd automatic-ecg-diagnosis
   ```

3. **Install dependencies:**
   ```bash
   pip install tensorflow==2.2 keras numpy flask flask-cors
   ```

4. **Download pre-trained model weights:**
   - Visit: https://doi.org/10.5281/zenodo.3625017
   - Download `model.hdf5`
   - Place in `Backend/automatic-ecg-diagnosis/model/`

5. **Run Flask server:**
   ```bash
   cd Backend
   python ecg_api.py
   # Server runs on http://localhost:5000
   ```

---

## ğŸ“š Documentation

- **[RESOURCES.md](RESOURCES.md)** - Complete list of resources, links, tutorials, and tools
- **[Dev Plan](dev/active/holohuman-xr/holohuman-xr-plan.md)** - 36-hour comprehensive development plan
- **[Context Doc](dev/active/holohuman-xr/holohuman-xr-context.md)** - Architecture, decisions, integration points
- **[Task Checklist](dev/active/holohuman-xr/holohuman-xr-tasks.md)** - Detailed task breakdown (165+ items)

---

## ğŸ‘¥ Team

- **Backend Lead** - Flask API, ML model integration
- **Backend Dev** - Asset preparation, Unity-Flask integration
- **Unity Lead** - VR interactions, layer system, scene polish
- **Unity Dev** - UI/UX, medical imaging, ECG visualization

---

## ğŸ† Hackathon Details

**Event:** Immerse the Bay 2025
**Dates:** November 14-16, 2025 (36 hours)
**Deadline:** Sunday, Nov 16 at 9:00 AM (HARD STOP)
**Theme:** "Down the Rabbit Hole"

**Requirements:**
- âœ… XR must be used in non-trivial way
- âœ… 30-second demo video (vertical format)
- âœ… Public GitHub repository with MIT license
- âœ… Complete Devpost submission
- âœ… Max $20 spending on paid services

---

## ğŸ¯ Roadmap

### MVP (Minimum Viable Product)
- [x] VR scene running on Quest 2
- [ ] Interactive heart model with peelable layers
- [ ] ECG visualization with AI analysis
- [ ] Basic medical imaging viewer
- [ ] Controller haptic feedback

### Stretch Goals
- [ ] Full skeleton model
- [ ] Hand tracking support
- [ ] Portal effect for scan space
- [ ] Multiple ECG analysis modes
- [ ] Voice commands

### Future Vision (Post-Hackathon)
- Real-time ECG from wearable devices
- Patient-specific anatomical models from DICOM segmentation
- Multi-user collaboration (doctors + patients)
- AI-powered automatic anomaly detection
- Integration with hospital PACS systems
- FDA-cleared medical device classification

---

## ğŸ¤ Contributing

This project was created for the Immerse the Bay 2025 Hackathon. Contributions are welcome post-hackathon!

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Unity MR Example Template** - https://github.com/Unity-Technologies/mr-example-meta-openxr
- **ECG Model** - [antonior92/automatic-ecg-diagnosis](https://github.com/antonior92/automatic-ecg-diagnosis) (Nature Communications 2020)
- **3D Models** - Z-Anatomy (Sketchfab), TurboSquid, Free3D
- **Medical Data** - MIT-BIH Arrhythmia Database, Medimodel
- **Immerse the Bay** - Stanford hackathon organizers

---

## ğŸ“§ Contact

GitHub: [@tyang4-ai](https://github.com/tyang4-ai)
Project Link: [https://github.com/tyang4-ai/Immerse-the-Bay-Hackathon](https://github.com/tyang4-ai/Immerse-the-Bay-Hackathon)

---

**Built with â¤ï¸ for Immerse the Bay 2025**

*"Step into the human body, peel back its layers, and explore real medical scans from the inside."*